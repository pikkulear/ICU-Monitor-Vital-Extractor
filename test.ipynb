{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCREEN DETECTION:\n",
    "-----------------\n",
    "YOLOv7: https://github.com/WongKinYiu/yolov7\n",
    "\n",
    "\n",
    "hyperparameters:\tlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, \n",
    "\t\t\twarmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, \n",
    "\t\t\tbox=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0,\n",
    "\t\t\tiou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7,\n",
    "\t\t\thsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0,\n",
    "\t\t\tperspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15,\n",
    "\t\t\tcopy_paste=0.0, paste_in=0.15, loss_ota=1\n",
    "\n",
    "\n",
    "image_size = 416\n",
    "Batch_size = 4\n",
    "No. of epochs = 10\n",
    "\n",
    "Final_precision: 0.93\n",
    "Final_recall: 0.97\n",
    "mAP@0.5: 0.995\n",
    "mAP@0.5:0.95: 0.96\n",
    "\n",
    "SCREEN CLASSIFIER:\n",
    "------------------\n",
    "\n",
    "Model used: mobilenetV3_small\n",
    "\n",
    "hyperparameters:\tlr=0.001, epochs=10, grad_clip=0.1, weight_decay=1e-4\n",
    "Optimizer used: SGD\n",
    "\n",
    "Final Validation Accuracy: 82%\n",
    "\n",
    "HEART RATE GRAPH DETECTION:\n",
    "---------------------------\n",
    "YOLOv5: https://github.com/ultralytics/yolov5\n",
    "\n",
    "hyperparameters:\tlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005,\n",
    "\t\t\twarmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1,\n",
    "\t\t\tbox=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0,\n",
    "\t\t\tiou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015,\n",
    "\t\t\thsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1,\n",
    "\t\t\tscale=0.5, shear=0.0, perspective=0.0, flipud=0.0,\n",
    "\t\t\tfliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
    "\n",
    "image_size: 416\n",
    "Batch size: 16\n",
    "No. of epochs: 10\n",
    "\n",
    "Final_precision: 0.95\n",
    "Final_recall: 0.97\n",
    "mAP@0.5: 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shres\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# requirements\n",
    "%pip install -q easyocr\n",
    "%pip install -q ocrd-fork-pylsd==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.test_dataset import LoadImages\n",
    "from perspective import Perspective\n",
    "from screen_ocr import ScreenOCR\n",
    "from models.experimental import attempt_load\n",
    "from utils.torch_utils import select_device\n",
    "from utils.general import non_max_suppression, scale_coords, xyxy2xywh\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipeline(object):\n",
    "    def __init__(self, weight, graph_weight=None,digitize_graph=False,device=''):\n",
    "        self.digitize_graph=False\n",
    "        if digitize_graph and graph_weight:\n",
    "            self.graphdetector=torch.load(graph_weight).to('cpu')\n",
    "            self.graphdetector.eval()\n",
    "            self.digitize_graph = True\n",
    "        self.device = select_device(device)\n",
    "        self.detection_model = attempt_load(weight, map_location=self.device)\n",
    "        self.stride = int(self.detection_model.stride.max())\n",
    "        self.classifier_model = None\n",
    "        self.ppt = Perspective()       # class for handling perspective change\n",
    "        self.ocr = ScreenOCR()         # class for handling OCR part\n",
    "        self.detection_model.eval()\n",
    "        \n",
    "        # self.classifier_model.eval()\n",
    "\n",
    "    def crop_bboxes(self, image, xywh, margin=15):\n",
    "        print(image.shape)\n",
    "        print(xywh)\n",
    "        h_img, w_img, _ = image.shape\n",
    "        x1 = int(max(0, xywh[0]*w_img-xywh[2]*w_img*0.5-margin))\n",
    "        y1 = int(max(0, xywh[1]*h_img-xywh[3]*h_img*0.5-margin))\n",
    "        x2 = int(min(w_img, xywh[0]*w_img + xywh[2]*w_img*0.5 + margin))\n",
    "        y2 = int(min(h_img, xywh[1]*h_img + xywh[3]*h_img*0.5 + margin))\n",
    "        cropped_image = image[y1:y2, x1:x2, :]\n",
    "        plt.imshow(cropped_image)\n",
    "        plt.show()\n",
    "        warped_image = self.ppt.shift_perspective(cropped_image)\n",
    "        return warped_image\n",
    "\n",
    "    def clean_img(self, img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # sharpen image\n",
    "        sharpen = cv2.GaussianBlur(gray, (0,0), 3)\n",
    "        sharpen = cv2.addWeighted(gray, 1.5, sharpen, -0.5, 0)\n",
    "\n",
    "        # apply adaptive threshold to get black and white effect\n",
    "        thresh = cv2.adaptiveThreshold(sharpen, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 15)\n",
    "        cv2.imwrite('thresh.jpeg',thresh)\n",
    "        return sharpen\n",
    "\n",
    "    def evaluate(self, test_data, classify=False):\n",
    "        # df = pd.DataFrame(columns=['rr', 'hr', 'spo2', 'map', 'sys', 'dia'])\n",
    "        df = []\n",
    "        for path, img, im0 in test_data:\n",
    "            img = torch.from_numpy(img).to(self.device).float()\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            if img.ndimension() == 3:\n",
    "                img = img.unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                pred = self.detection_model(img, augment=True)[0]\n",
    "            pred = non_max_suppression(pred, 0.25, 0.45, agnostic=True)\n",
    "            det = pred[0]\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "            cropped_image = self.crop_bboxes(im0, xywh)\n",
    "            cv2.imwrite('cropped.jpeg',cropped_image)\n",
    "            if classify:\n",
    "                with torch.no_grad():\n",
    "                    screen_types = self.classifier_model(cropped_image)\n",
    "            if self.digitize_graph:\n",
    "                with torch.no_grad():\n",
    "                    pred=self.graphdetector(img, augment=True)\n",
    "                pred = non_max_suppression(pred, 0.25, 0.45, agnostic=True)\n",
    "                det = pred[0]\n",
    "                if len(det):\n",
    "                    # Rescale boxes from img_size to im0 size\n",
    "                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                    for *xyxy, conf, cls in reversed(det):\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                cropped_graph = self.crop_bboxes(im0, xywh)\n",
    "                cv2.imwrite('cropped_graph.jpeg',cropped_graph)\n",
    "            # cleaning and OCR part\n",
    "            img = cropped_image.copy()\n",
    "            cleaned = self.clean_img(cropped_image)\n",
    "            vitals_dict = self.ocr.read_vitals(image=cleaned, image_rgb=img)\n",
    "            print(vitals_dict)\n",
    "            df.append(vitals_dict)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLOv7_WEIGHT  = './weights/yolov7_best.pt'\n",
    "\n",
    "def inference(image_path:str):\n",
    "    \"\"\"\n",
    "    Function responsible for inference.\n",
    "    Args: \n",
    "      image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
    "    Returns:\n",
    "      result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
    "    \"\"\"\n",
    "  \n",
    "    ### put your code here\n",
    "    t0=time.time()\n",
    "    fpl = FullPipeline(YOLOv7_WEIGHT)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dataset = LoadImages(image_path, img_size=640, stride=fpl.stride)\n",
    "\n",
    "    t2 = time.time()\n",
    "    df = fpl.evaluate(dataset)\n",
    "\n",
    "    result = df[0]\n",
    "\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "(720, 1280, 3)\n",
      "[0.4546875059604645, 0.46666666865348816, 0.38593751192092896, 0.5416666865348816]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shres\\AppData\\Local\\Temp/ipykernel_2660/956786439.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RR': 26, 'HR': 99, 'SPO2': None, 'MAP': None, 'SBP': None, 'DBP': None}\n"
     ]
    }
   ],
   "source": [
    "res = inference('images/cchdavangere_micu_mon--2_2023_1_5_5_10_1.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RR': 26, 'HR': 99, 'SPO2': None, 'MAP': None, 'SBP': None, 'DBP': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f51649b3ec5c911d0351ee6f08b4512727ebdf4d91c2a19dc07c517629d772a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
